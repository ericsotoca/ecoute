<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>V2- Voice2Text Long Session - Édition segmentée</title>
<style>
  body { font-family: Arial, sans-serif; padding: 20px; max-width: 700px; margin: auto; }
  button { padding: 10px 20px; margin: 5px; font-size: 16px; }
  textarea { width: 100%; height: 400px; margin-top: 10px; font-size: 16px; }
  h1 { color: #333; }
</style>
</head>
<body>

<h1>Voice2Text Long Session (mode segmenté)</h1>

<button id="startBtn">Démarrer l'enregistrement</button>
<button id="stopBtn" disabled>Arrêter</button>
<button id="saveBtn" disabled>Sauvegarder le texte</button>

<p id="status" style="color:green;font-weight:bold;"></p>
<textarea id="transcription" placeholder="Le texte transcrit apparaîtra ici..."></textarea>

<script>
let recognition;
let isRecording = false;
let fullTranscript = localStorage.getItem("fullTranscript") || '';
let segmentTimer;
let mediaRecorder;
let audioChunks = [];
const segmentDuration = 5 * 60 * 1000; // 5 minutes en millisecondes

const transcriptionArea = document.getElementById('transcription');
const statusEl = document.getElementById('status');
transcriptionArea.value = fullTranscript;

// === INITIALISATION RECONNAISSANCE VOCALE ===
if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  recognition = new SpeechRecognition();
  recognition.continuous = true;
  recognition.interimResults = true;
  recognition.lang = 'fr-FR';

  recognition.onresult = function(event) {
    let interimTranscript = '';
    for (let i = event.resultIndex; i < event.results.length; i++) {
      interimTranscript += event.results[i][0].transcript;
    }
    transcriptionArea.value = fullTranscript + interimTranscript;
  };

  recognition.onend = function() {
    if (isRecording) {
      fullTranscript += transcriptionArea.value.slice(fullTranscript.length);
      recognition.start(); // redémarre la session automatiquement
    }
  };
} else {
  alert("Votre navigateur ne supporte pas la Web Speech API.");
}

// === BOUTONS ===
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const saveBtn = document.getElementById('saveBtn');

startBtn.onclick = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(stream);
    audioChunks = [];

    mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
    mediaRecorder.onstop = handleSegmentStop;

    mediaRecorder.start();
    recognition.start();
    isRecording = true;
    statusEl.textContent = "Enregistrement en cours...";

    startBtn.disabled = true;
    stopBtn.disabled = false;
    saveBtn.disabled = true;

    // Déclenche le découpage automatique des segments
    segmentTimer = setInterval(() => {
      if (isRecording) {
        mediaRecorder.stop(); // stoppe le segment
        mediaRecorder.start(); // relance un nouveau segment
      }
    }, segmentDuration);
  } catch (err) {
    alert("Erreur d'accès au micro : " + err.message);
  }
};

stopBtn.onclick = () => {
  isRecording = false;
  recognition.stop();
  clearInterval(segmentTimer);
  if (mediaRecorder && mediaRecorder.state !== "inactive") {
    mediaRecorder.stop();
  }
  fullTranscript += transcriptionArea.value.slice(fullTranscript.length);
  localStorage.setItem("fullTranscript", fullTranscript);
  startBtn.disabled = false;
  stopBtn.disabled = true;
  saveBtn.disabled = false;
  statusEl.textContent = "Enregistrement arrêté.";
};

saveBtn.onclick = () => {
  const text = transcriptionArea.value;
  const blob = new Blob([text], { type: "text/plain" });
  const link = document.createElement('a');
  link.href = URL.createObjectURL(blob);
  link.download = 'transcription.txt';
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
};

// === TRAITEMENT D'UN SEGMENT AUDIO ===
async function handleSegmentStop() {
  const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
  audioChunks = []; // libère la mémoire

  // Simulation de transcription segmentée :
  // on pourrait envoyer audioBlob à un moteur externe (Whisper)
  // Ici on se contente d'ajouter un marqueur
  fullTranscript += "\n\n--- Segment de 5 min terminé ---\n";
  transcriptionArea.value = fullTranscript;
  localStorage.setItem("fullTranscript", fullTranscript);

  // Efface le blob du segment pour libérer l’espace
  URL.revokeObjectURL(audioBlob);
}

// === SAUVEGARDE AUTO TOUTES LES 5s ===
setInterval(() => {
  if (fullTranscript.length > 0) {
    fullTranscript = transcriptionArea.value;
    localStorage.setItem("fullTranscript", fullTranscript);
  }
}, 5000);
</script>

</body>
</html>
